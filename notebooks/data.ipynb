{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "          id                                             review  sentiment\n0          0  Action & Adventure.Billie Clark is twenty year...          1\n1          1  I found this early talkie difficult to watch a...          0\n2          2  It was Libby talking to Desmond in the flashba...          1\n3          3  We fans of Ed Wood tend to be an obsessive bun...          1\n4          4  Hmm, Hip Hop music to a period western. Modern...          0\n...      ...                                                ...        ...\n39577  39577  This story had a different angle that intrigue...          0\n39578  39578  I thought the movie (especially the plot) need...          0\n39579  39579  The title of this film nearly put me off watch...          1\n39580  39580  Mmm, doesn't a big stack of pancakes sound goo...          0\n39581  39581  I enjoyed this film. I thought it was an excel...          1\n\n[39582 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Action &amp; Adventure.Billie Clark is twenty year...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>I found this early talkie difficult to watch a...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>It was Libby talking to Desmond in the flashba...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>We fans of Ed Wood tend to be an obsessive bun...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Hmm, Hip Hop music to a period western. Modern...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>39577</th>\n      <td>39577</td>\n      <td>This story had a different angle that intrigue...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>39578</th>\n      <td>39578</td>\n      <td>I thought the movie (especially the plot) need...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>39579</th>\n      <td>39579</td>\n      <td>The title of this film nearly put me off watch...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>39580</th>\n      <td>39580</td>\n      <td>Mmm, doesn't a big stack of pancakes sound goo...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>39581</th>\n      <td>39581</td>\n      <td>I enjoyed this film. I thought it was an excel...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>39582 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ds = pd.read_csv('../data/train.csv')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "          id                                             review  sentiment\n29981  29981  I watched this movie at the first showing avai...          1\n34765  34765  A clever, undeniably entertaining romp starrin...          1\n32373  32373  In 1984, The Karate Kid had some charm to it, ...          0\n5759    5759  Along with \"Brothers & Sisters\", \"Six Degrees\"...          1\n25572  25572  I was watching this movie at one of my usual t...          1\n...      ...                                                ...        ...\n35844  35844  I watched this film with my family over a long...          1\n23196  23196  This is a must see for independant movie fans,...          1\n38287  38287  This is a very unusual film in that the star w...          1\n15595  15595  I have watched this movie a few times and neve...          0\n20931  20931  Every Sunday is an eleven minute short subject...          1\n\n[6000 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>29981</th>\n      <td>29981</td>\n      <td>I watched this movie at the first showing avai...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>34765</th>\n      <td>34765</td>\n      <td>A clever, undeniably entertaining romp starrin...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>32373</th>\n      <td>32373</td>\n      <td>In 1984, The Karate Kid had some charm to it, ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5759</th>\n      <td>5759</td>\n      <td>Along with \"Brothers &amp; Sisters\", \"Six Degrees\"...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>25572</th>\n      <td>25572</td>\n      <td>I was watching this movie at one of my usual t...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>35844</th>\n      <td>35844</td>\n      <td>I watched this film with my family over a long...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>23196</th>\n      <td>23196</td>\n      <td>This is a must see for independant movie fans,...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>38287</th>\n      <td>38287</td>\n      <td>This is a very unusual film in that the star w...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>15595</th>\n      <td>15595</td>\n      <td>I have watched this movie a few times and neve...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>20931</th>\n      <td>20931</td>\n      <td>Every Sunday is an eleven minute short subject...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>6000 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ds_sample, _ = train_test_split(ds, train_size=6000)\n",
    "ds_sample"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "'A clever, undeniably entertaining romp starring Peter Ustinov as a career embezzler with his sights set on a US conglomerate in London. He\\'s abetted by foxy (and deceptively sharp) Maggie Smith and threatened with exposure by jealous company man Bob Newhart. This is a heist film with a lot of brains. Ustinov is exceptional as is Smith and Newhart is quite funny. The real surprise is Karl Malden, as the pill popping \"executive vice-president.\" Malden has seldom been so loose. Cesar Romero and Robert Morley pop up in some funny cameos. The excellent music score is by Laurie Johnson. Eric Till\\'s direction is brisk and the script by Ustinov and Ira Wallach is first-rate & very smart. A swinging good time!'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = ds_sample.review.iloc[1]\n",
    "txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nikita_gordia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "['A',\n 'clever',\n ',',\n 'undeniably',\n 'entertaining',\n 'romp',\n 'starring',\n 'Peter',\n 'Ustinov',\n 'as',\n 'a',\n 'career',\n 'embezzler',\n 'with',\n 'his',\n 'sights',\n 'set',\n 'on',\n 'a',\n 'US',\n 'conglomerate',\n 'in',\n 'London',\n '.',\n 'He',\n \"'s\",\n 'abetted',\n 'by',\n 'foxy',\n '(',\n 'and',\n 'deceptively',\n 'sharp',\n ')',\n 'Maggie',\n 'Smith',\n 'and',\n 'threatened',\n 'with',\n 'exposure',\n 'by',\n 'jealous',\n 'company',\n 'man',\n 'Bob',\n 'Newhart',\n '.',\n 'This',\n 'is',\n 'a',\n 'heist',\n 'film',\n 'with',\n 'a',\n 'lot',\n 'of',\n 'brains',\n '.',\n 'Ustinov',\n 'is',\n 'exceptional',\n 'as',\n 'is',\n 'Smith',\n 'and',\n 'Newhart',\n 'is',\n 'quite',\n 'funny',\n '.',\n 'The',\n 'real',\n 'surprise',\n 'is',\n 'Karl',\n 'Malden',\n ',',\n 'as',\n 'the',\n 'pill',\n 'popping',\n '``',\n 'executive',\n 'vice-president',\n '.',\n \"''\",\n 'Malden',\n 'has',\n 'seldom',\n 'been',\n 'so',\n 'loose',\n '.',\n 'Cesar',\n 'Romero',\n 'and',\n 'Robert',\n 'Morley',\n 'pop',\n 'up',\n 'in',\n 'some',\n 'funny',\n 'cameos',\n '.',\n 'The',\n 'excellent',\n 'music',\n 'score',\n 'is',\n 'by',\n 'Laurie',\n 'Johnson',\n '.',\n 'Eric',\n 'Till',\n \"'s\",\n 'direction',\n 'is',\n 'brisk',\n 'and',\n 'the',\n 'script',\n 'by',\n 'Ustinov',\n 'and',\n 'Ira',\n 'Wallach',\n 'is',\n 'first-rate',\n '&',\n 'very',\n 'smart',\n '.',\n 'A',\n 'swinging',\n 'good',\n 'time',\n '!']"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(txt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "'A clever undeniably entertaining romp starring Peter Ustinov as a career embezzler with his sights set on a US conglomerate in London Hes abetted by foxy and deceptively sharp Maggie Smith and threatened with exposure by jealous company man Bob Newhart This is a heist film with a lot of brains Ustinov is exceptional as is Smith and Newhart is quite funny The real surprise is Karl Malden as the pill popping executive vicepresident Malden has seldom been so loose Cesar Romero and Robert Morley pop up in some funny cameos The excellent music score is by Laurie Johnson Eric Tills direction is brisk and the script by Ustinov and Ira Wallach is firstrate  very smart A swinging good time'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuation(str):\n",
    "    return \"\".join([c for c in str if c not in string.punctuation])\n",
    "\n",
    "def remove_numbers(str):\n",
    "    return \"\".join([c for c in str if c not in string.digits])\n",
    "\n",
    "txt1 = remove_punctuation(txt)\n",
    "txt1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "['clever',\n 'undeniably',\n 'entertaining',\n 'romp',\n 'starring',\n 'peter',\n 'ustinov',\n 'career',\n 'embezzler',\n 'sights',\n 'set',\n 'us',\n 'conglomerate',\n 'london',\n 'hes',\n 'abetted',\n 'foxy',\n 'deceptively',\n 'sharp',\n 'maggie',\n 'smith',\n 'threatened',\n 'exposure',\n 'jealous',\n 'company',\n 'man',\n 'bob',\n 'newhart',\n 'heist',\n 'film',\n 'lot',\n 'brains',\n 'ustinov',\n 'exceptional',\n 'smith',\n 'newhart',\n 'quite',\n 'funny',\n 'real',\n 'surprise',\n 'karl',\n 'malden',\n 'pill',\n 'popping',\n 'executive',\n 'vicepresident',\n 'malden',\n 'seldom',\n 'loose',\n 'cesar',\n 'romero',\n 'robert',\n 'morley',\n 'pop',\n 'funny',\n 'cameos',\n 'excellent',\n 'music',\n 'score',\n 'laurie',\n 'johnson',\n 'eric',\n 'tills',\n 'direction',\n 'brisk',\n 'script',\n 'ustinov',\n 'ira',\n 'wallach',\n 'firstrate',\n 'smart',\n 'swinging',\n 'good',\n 'time']"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def clean_stopwords(words):\n",
    "    stop = stopwords.words('english')\n",
    "    return [w for w in words if w not in stop]\n",
    "\n",
    "def tokenize(txt):\n",
    "    return nltk.word_tokenize(txt)\n",
    "\n",
    "def lower(txt):\n",
    "    return txt.lower()\n",
    "\n",
    "def replace_numbers(words):\n",
    "    return [(\"somenumber\" if any(ch.isdigit() for ch in w) else w) for w in words]\n",
    "\n",
    "txt2 = replace_numbers(clean_stopwords(tokenize(lower(txt1))))\n",
    "txt2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "['clever',\n 'undeniably',\n 'entertaining',\n 'romp',\n 'starring',\n 'peter',\n 'ustinov',\n 'career',\n 'embezzler',\n 'sight',\n 'set',\n 'u',\n 'conglomerate',\n 'london',\n 'he',\n 'abetted',\n 'foxy',\n 'deceptively',\n 'sharp',\n 'maggie',\n 'smith',\n 'threatened',\n 'exposure',\n 'jealous',\n 'company',\n 'man',\n 'bob',\n 'newhart',\n 'heist',\n 'film',\n 'lot',\n 'brain',\n 'ustinov',\n 'exceptional',\n 'smith',\n 'newhart',\n 'quite',\n 'funny',\n 'real',\n 'surprise',\n 'karl',\n 'malden',\n 'pill',\n 'popping',\n 'executive',\n 'vicepresident',\n 'malden',\n 'seldom',\n 'loose',\n 'cesar',\n 'romero',\n 'robert',\n 'morley',\n 'pop',\n 'funny',\n 'cameo',\n 'excellent',\n 'music',\n 'score',\n 'laurie',\n 'johnson',\n 'eric',\n 'till',\n 'direction',\n 'brisk',\n 'script',\n 'ustinov',\n 'ira',\n 'wallach',\n 'firstrate',\n 'smart',\n 'swinging',\n 'good',\n 'time']"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lem(words):\n",
    "    lemmer = nltk.WordNetLemmatizer()\n",
    "    return [lemmer.lemmatize(word) for word in words]\n",
    "\n",
    "def stem(words):\n",
    "    stemmer = nltk.stem.snowball.EnglishStemmer()\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "lem(txt2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8343333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def eval_data(*txt_fn, dataset=ds_sample, max_features=None):\n",
    "    def prep_txt(txt):\n",
    "        res = txt\n",
    "        for fn in txt_fn:\n",
    "            res = fn(res)\n",
    "        return res\n",
    "\n",
    "    count = CountVectorizer(analyzer=prep_txt, max_features=max_features)\n",
    "    X = count.fit_transform(dataset.review)\n",
    "    model = MultinomialNB()\n",
    "    print(cross_val_score(model, X, dataset.sentiment).mean())\n",
    "\n",
    "eval_data(\n",
    "    remove_punctuation,\n",
    "    remove_numbers,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    clean_stopwords,\n",
    "    stem,\n",
    "    clean_stopwords,\n",
    "    lem,\n",
    "    clean_stopwords\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.834\n"
     ]
    }
   ],
   "source": [
    "eval_data(\n",
    "    remove_punctuation,\n",
    "    remove_numbers,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    clean_stopwords,\n",
    "    lem,\n",
    "    clean_stopwords\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8323333333333334\n"
     ]
    }
   ],
   "source": [
    "eval_data(\n",
    "    remove_punctuation,\n",
    "    remove_numbers,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    clean_stopwords,\n",
    "    stem,\n",
    "    clean_stopwords\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8331666666666667\n"
     ]
    }
   ],
   "source": [
    "eval_data(\n",
    "    remove_punctuation,\n",
    "    remove_numbers,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    clean_stopwords,\n",
    "    lem,\n",
    "    clean_stopwords,\n",
    "    stem,\n",
    "    clean_stopwords\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8233333333333335\n"
     ]
    }
   ],
   "source": [
    "eval_data(\n",
    "    remove_punctuation,\n",
    "    remove_numbers,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    lem,\n",
    "    stem\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.828\n"
     ]
    }
   ],
   "source": [
    "eval_data(\n",
    "    lower,\n",
    "    tokenize,\n",
    "    clean_stopwords,\n",
    "    stem,\n",
    "    clean_stopwords,\n",
    "    lem,\n",
    "    clean_stopwords\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8360000000000001\n"
     ]
    }
   ],
   "source": [
    "eval_data(\n",
    "    remove_punctuation,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    clean_stopwords,\n",
    "    stem,\n",
    "    clean_stopwords,\n",
    "    lem,\n",
    "    clean_stopwords\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8333333333333333\n"
     ]
    }
   ],
   "source": [
    "eval_data(\n",
    "    remove_punctuation,\n",
    "    remove_numbers,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    clean_stopwords,\n",
    "    stem,\n",
    "    lem\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### No stem\n",
    "Best: 0.834\n",
    "\n",
    "eval_data(\n",
    "    remove_punctuation,\n",
    "    remove_numbers,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    clean_stopwords,\n",
    "    lem,\n",
    "    clean_stopwords\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8338333333333333\n"
     ]
    }
   ],
   "source": [
    "eval_data(\n",
    "    remove_punctuation,\n",
    "    remove_numbers,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    clean_stopwords,\n",
    "    lem\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8258333333333333\n"
     ]
    }
   ],
   "source": [
    "eval_data(\n",
    "    remove_punctuation,\n",
    "    remove_numbers,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    lem\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8370000000000001\n"
     ]
    }
   ],
   "source": [
    "eval_data(\n",
    "    remove_punctuation,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    clean_stopwords,\n",
    "    lem,\n",
    "    clean_stopwords\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8283333333333334\n"
     ]
    }
   ],
   "source": [
    "eval_data(\n",
    "    remove_numbers,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    clean_stopwords,\n",
    "    lem,\n",
    "    clean_stopwords\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8338333333333333\n"
     ]
    }
   ],
   "source": [
    "eval_data(\n",
    "    remove_punctuation,\n",
    "    remove_numbers,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    clean_stopwords,\n",
    "    lem\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### No remove numbers\n",
    "\n",
    "0.8370000000000001\n",
    "\n",
    "eval_data(\n",
    "    remove_punctuation,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    clean_stopwords,\n",
    "    lem,\n",
    "    clean_stopwords\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8335000000000001\n"
     ]
    }
   ],
   "source": [
    "eval_data(\n",
    "    remove_punctuation,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    replace_numbers,\n",
    "    clean_stopwords,\n",
    "    lem,\n",
    "    clean_stopwords\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test max features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8401666666666667\n"
     ]
    }
   ],
   "source": [
    "eval_data(\n",
    "    remove_punctuation,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    clean_stopwords,\n",
    "    lem,\n",
    "    clean_stopwords,\n",
    "    max_features=2000\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8388333333333333\n"
     ]
    }
   ],
   "source": [
    "eval_data(\n",
    "    remove_punctuation,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    clean_stopwords,\n",
    "    lem,\n",
    "    clean_stopwords,\n",
    "    max_features=2200\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8400000000000001\n"
     ]
    }
   ],
   "source": [
    "eval_data(\n",
    "    remove_punctuation,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    clean_stopwords,\n",
    "    lem,\n",
    "    clean_stopwords,\n",
    "    max_features=1700\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test n-grams"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl = nltk.WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return clean_stopwords(lem(clean_stopwords(tokenize(doc))))\n",
    "\n",
    "def preprocessor(input):\n",
    "    return lower(remove_punctuation(input))\n",
    "\n",
    "# count = CountVectorizer(tokenizer=LemmaTokenizer(),\n",
    "#                         preprocessor=preprocessor,\n",
    "#                         max_features=4500,\n",
    "#                         ngram_range=(1, 3))\n",
    "# X = count.fit_transform(ds_sample.review)\n",
    "# model = MultinomialNB()\n",
    "# print(cross_val_score(model, X, ds_sample.sentiment).mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8471666666666666\n"
     ]
    }
   ],
   "source": [
    "count = CountVectorizer(tokenizer=LemmaTokenizer(),\n",
    "                        preprocessor=preprocessor,\n",
    "                        max_features=4800,\n",
    "                        ngram_range=(1, 3))\n",
    "X = count.fit_transform(ds_sample.review)\n",
    "model = MultinomialNB()\n",
    "print(cross_val_score(model, X, ds_sample.sentiment).mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8468333333333333\n"
     ]
    }
   ],
   "source": [
    "count = CountVectorizer(tokenizer=LemmaTokenizer(),\n",
    "                        preprocessor=preprocessor,\n",
    "                        max_features=5500,\n",
    "                        ngram_range=(1, 3))\n",
    "X = count.fit_transform(ds_sample.review)\n",
    "model = MultinomialNB()\n",
    "print(cross_val_score(model, X, ds_sample.sentiment).mean())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8465\n"
     ]
    }
   ],
   "source": [
    "\n",
    "count = CountVectorizer(tokenizer=LemmaTokenizer(),\n",
    "                        preprocessor=preprocessor,\n",
    "                        max_features=5300,\n",
    "                        ngram_range=(1, 3))\n",
    "X = count.fit_transform(ds_sample.review)\n",
    "model = MultinomialNB()\n",
    "print(cross_val_score(model, X, ds_sample.sentiment).mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8526666666666667\n"
     ]
    }
   ],
   "source": [
    "count = CountVectorizer(tokenizer=LemmaTokenizer(),\n",
    "                        preprocessor=preprocessor,\n",
    "                        max_features=5400,\n",
    "                        ngram_range=(1, 3))\n",
    "X = count.fit_transform(ds_sample.review)\n",
    "model = MultinomialNB()\n",
    "print(cross_val_score(model, X, ds_sample.sentiment).mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def eval_data_tfidf(*txt_fn, dataset=ds_sample, max_features=None):\n",
    "    def prep_txt(txt):\n",
    "        res = txt\n",
    "        for fn in txt_fn:\n",
    "            res = fn(res)\n",
    "        return res\n",
    "\n",
    "    count = TfidfVectorizer(analyzer=prep_txt, max_features=max_features)\n",
    "    X = count.fit_transform(dataset.review)\n",
    "    model = MultinomialNB()\n",
    "    print(cross_val_score(model, X, dataset.sentiment).mean())\n",
    "\n",
    "# eval_data_tfidf(\n",
    "#     remove_punctuation,\n",
    "#     remove_numbers,\n",
    "#     lower,\n",
    "#     tokenize,\n",
    "#     clean_stopwords,\n",
    "#     stem,\n",
    "#     clean_stopwords,\n",
    "#     lem,\n",
    "#     clean_stopwords\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8486666666666667\n"
     ]
    }
   ],
   "source": [
    "eval_data_tfidf(\n",
    "    remove_punctuation,\n",
    "    remove_numbers,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    clean_stopwords,\n",
    "    lem,\n",
    "    clean_stopwords\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.845\n"
     ]
    }
   ],
   "source": [
    "eval_data_tfidf(\n",
    "    remove_punctuation,\n",
    "    remove_numbers,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    clean_stopwords,\n",
    "    stem,\n",
    "    clean_stopwords\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.845\n"
     ]
    }
   ],
   "source": [
    "eval_data_tfidf(\n",
    "    remove_punctuation,\n",
    "    remove_numbers,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    clean_stopwords,\n",
    "    lem,\n",
    "    clean_stopwords,\n",
    "    stem,\n",
    "    clean_stopwords\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8351666666666666\n"
     ]
    }
   ],
   "source": [
    "eval_data_tfidf(\n",
    "    remove_punctuation,\n",
    "    remove_numbers,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    lem,\n",
    "    stem\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8431666666666666\n"
     ]
    }
   ],
   "source": [
    "eval_data_tfidf(\n",
    "    lower,\n",
    "    tokenize,\n",
    "    clean_stopwords,\n",
    "    stem,\n",
    "    clean_stopwords,\n",
    "    lem,\n",
    "    clean_stopwords\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8448333333333334\n"
     ]
    }
   ],
   "source": [
    "eval_data_tfidf(\n",
    "    remove_punctuation,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    clean_stopwords,\n",
    "    stem,\n",
    "    clean_stopwords,\n",
    "    lem,\n",
    "    clean_stopwords\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.844\n"
     ]
    }
   ],
   "source": [
    "eval_data_tfidf(\n",
    "    remove_punctuation,\n",
    "    remove_numbers,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    clean_stopwords,\n",
    "    stem,\n",
    "    lem\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### No stem\n",
    "Best: 0.8486666666666667\n",
    "\n",
    "eval_data_tfidf(\n",
    "    remove_punctuation,\n",
    "    remove_numbers,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    clean_stopwords,\n",
    "    lem,\n",
    "    clean_stopwords\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8486666666666667\n"
     ]
    }
   ],
   "source": [
    "eval_data_tfidf(\n",
    "    remove_punctuation,\n",
    "    remove_numbers,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    clean_stopwords,\n",
    "    lem\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8393333333333335\n"
     ]
    }
   ],
   "source": [
    "eval_data_tfidf(\n",
    "    remove_punctuation,\n",
    "    remove_numbers,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    lem\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.851\n"
     ]
    }
   ],
   "source": [
    "eval_data_tfidf(\n",
    "    remove_punctuation,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    clean_stopwords,\n",
    "    lem,\n",
    "    clean_stopwords\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8440000000000001\n"
     ]
    }
   ],
   "source": [
    "eval_data_tfidf(\n",
    "    remove_numbers,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    clean_stopwords,\n",
    "    lem,\n",
    "    clean_stopwords\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8486666666666667\n"
     ]
    }
   ],
   "source": [
    "eval_data_tfidf(\n",
    "    remove_punctuation,\n",
    "    remove_numbers,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    clean_stopwords,\n",
    "    lem\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### No remove numbers\n",
    "\n",
    "0.851\n",
    "\n",
    "eval_data_tfidf(\n",
    "    remove_punctuation,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    clean_stopwords,\n",
    "    lem,\n",
    "    clean_stopwords\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8485000000000001\n"
     ]
    }
   ],
   "source": [
    "eval_data_tfidf(\n",
    "    remove_punctuation,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    replace_numbers,\n",
    "    clean_stopwords,\n",
    "    lem,\n",
    "    clean_stopwords\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test max features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8358333333333334\n"
     ]
    }
   ],
   "source": [
    "eval_data_tfidf(\n",
    "    remove_punctuation,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    clean_stopwords,\n",
    "    lem,\n",
    "    clean_stopwords,\n",
    "    max_features=1000\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8445\n"
     ]
    }
   ],
   "source": [
    "eval_data_tfidf(\n",
    "    remove_punctuation,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    clean_stopwords,\n",
    "    lem,\n",
    "    clean_stopwords,\n",
    "    max_features=2000\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8478333333333333\n"
     ]
    }
   ],
   "source": [
    "eval_data_tfidf(\n",
    "    remove_punctuation,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    clean_stopwords,\n",
    "    lem,\n",
    "    clean_stopwords,\n",
    "    max_features=3000\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8463333333333335\n"
     ]
    }
   ],
   "source": [
    "eval_data_tfidf(\n",
    "    remove_punctuation,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    clean_stopwords,\n",
    "    lem,\n",
    "    clean_stopwords,\n",
    "    max_features=4000\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8476666666666667\n"
     ]
    }
   ],
   "source": [
    "eval_data_tfidf(\n",
    "    remove_punctuation,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    clean_stopwords,\n",
    "    lem,\n",
    "    clean_stopwords,\n",
    "    max_features=5000\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8468333333333333\n"
     ]
    }
   ],
   "source": [
    "eval_data_tfidf(\n",
    "    remove_punctuation,\n",
    "    lower,\n",
    "    tokenize,\n",
    "    clean_stopwords,\n",
    "    lem,\n",
    "    clean_stopwords,\n",
    "    max_features=6000\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test n-grams for TDIDF"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8641666666666665\n"
     ]
    }
   ],
   "source": [
    "\n",
    "count = TfidfVectorizer(tokenizer=LemmaTokenizer(),\n",
    "                        preprocessor=preprocessor,\n",
    "                        ngram_range=(1, 2))\n",
    "X = count.fit_transform(ds_sample.review)\n",
    "model = MultinomialNB()\n",
    "print(cross_val_score(model, X, ds_sample.sentiment).mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8651666666666668\n"
     ]
    }
   ],
   "source": [
    "count = TfidfVectorizer(tokenizer=LemmaTokenizer(),\n",
    "                        preprocessor=preprocessor,\n",
    "                        ngram_range=(1, 3))\n",
    "X = count.fit_transform(ds_sample.review)\n",
    "model = MultinomialNB()\n",
    "print(cross_val_score(model, X, ds_sample.sentiment).mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.865\n"
     ]
    }
   ],
   "source": [
    "count = TfidfVectorizer(tokenizer=LemmaTokenizer(),\n",
    "                        preprocessor=preprocessor,\n",
    "                        ngram_range=(1, 4))\n",
    "X = count.fit_transform(ds_sample.review)\n",
    "model = MultinomialNB()\n",
    "print(cross_val_score(model, X, ds_sample.sentiment).mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 5408)\n",
      "0.853\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "count = TfidfVectorizer(tokenizer=LemmaTokenizer(),\n",
    "                        preprocessor=preprocessor,\n",
    "                        min_df=20,\n",
    "                        ngram_range=(1, 2))\n",
    "X = count.fit_transform(ds_sample.review)\n",
    "print(X.shape)\n",
    "model = MultinomialNB()\n",
    "print(cross_val_score(model, X, ds_sample.sentiment).mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 5442)\n",
      "0.853\n"
     ]
    }
   ],
   "source": [
    "count = TfidfVectorizer(tokenizer=LemmaTokenizer(),\n",
    "                        preprocessor=preprocessor,\n",
    "                        min_df=20,\n",
    "                        ngram_range=(1, 5))\n",
    "X = count.fit_transform(ds_sample.review)\n",
    "print(X.shape)\n",
    "model = MultinomialNB()\n",
    "print(cross_val_score(model, X, ds_sample.sentiment).mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 5441)\n",
      "0.8531666666666666\n"
     ]
    }
   ],
   "source": [
    "count = TfidfVectorizer(tokenizer=LemmaTokenizer(),\n",
    "                        preprocessor=preprocessor,\n",
    "                        min_df=20,\n",
    "                        ngram_range=(1, 4))\n",
    "X = count.fit_transform(ds_sample.review)\n",
    "print(X.shape)\n",
    "model = MultinomialNB()\n",
    "print(cross_val_score(model, X, ds_sample.sentiment).mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 2344)\n",
      "0.8466666666666667\n"
     ]
    }
   ],
   "source": [
    "count = TfidfVectorizer(tokenizer=LemmaTokenizer(),\n",
    "                        preprocessor=preprocessor,\n",
    "                        min_df=40)\n",
    "X = count.fit_transform(ds_sample.review)\n",
    "print(X.shape)\n",
    "model = MultinomialNB()\n",
    "print(cross_val_score(model, X, ds_sample.sentiment).mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "count = CountVectorizer(tokenizer=LemmaTokenizer(),\n",
    "                        preprocessor=preprocessor,\n",
    "                        max_features=5400,\n",
    "                        ngram_range=(1, 3))\n",
    "\n",
    "np.savez_compressed('../data/dev/feed_count',\n",
    "        count.fit_transform(ds.review).toarray())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=LemmaTokenizer(),\n",
    "                        preprocessor=preprocessor,\n",
    "                        max_features=20000,\n",
    "                        ngram_range=(1, 3))\n",
    "\n",
    "np.save('../data/dev/feed_tfidf.npy',\n",
    "        tfidf.fit_transform(ds.review).toarray())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready to save\n",
      "(39582, 33559)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "count = TfidfVectorizer(tokenizer=LemmaTokenizer(),\n",
    "                        preprocessor=preprocessor,\n",
    "                        min_df=20,\n",
    "                        ngram_range=(1, 4))\n",
    "\n",
    "print('ready to save')\n",
    "array = tfidf.fit_transform(ds.review).toarray()\n",
    "print(array.shape)\n",
    "\n",
    "np.savez_compressed('../data/dev/feed_tfidf_lite', array)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}